print(predictions)
#Fit the multiple linear regression model for Age and FEV2
model4 <- lm(PEmax ~ Age + FEV2, data = cf2)
#Create a new data frame with Age 16 and FEV2 levels
new_df <- data.frame(Age = rep(16, 3), FEV2 = factor(c(1, 2, 3), levels = levels(cf2$FEV2)))
#Predict the PEmax scores using the model
predictions <- predict(model4, newdata = new_df)
#Print the predictions
print(predictions)
#Fit the model with PEmax, Age, FEV2, and Age and FEV2 interactions
model5 <- lm(PEmax ~ Age + FEV2 + Age:FEV2, data = cf2)
#Print the summary of the model
summary(model5)
#Assign the values for Age and FEV2
age <- 16
fev2_1 <- 1
fev2_2 <- 2
fev2_3 <- 3
#Calculate the expected PEmax scores using the model equation
expected_score_1 <- coef(model5)["(Intercept)"] +
coef(model5)["Age"] * age +
coef(model5)["FEV2FEV22"] * fev2_1 +
coef(model5)["FEV2FEV23"] * fev2_1 * age
expected_score_2 <- coef(model5)["(Intercept)"] +
coef(model5)["Age"] * age +
coef(model5)["FEV2FEV22"] * fev2_2 +
coef(model5)["FEV2FEV23"] * fev2_2 * age
expected_score_3 <- coef(model5)["(Intercept)"] +
coef(model5)["Age"] * age +
coef(model5)["FEV2FEV22"] * fev2_3 +
coef(model5)["FEV2FEV23"] * fev2_3 * age
#Print the expected scores
print(expected_score_1)
print(expected_score_2)
print(expected_score_3)
#Predict the PEmax scores using the model
predictions2 <- predict(model5, newdata = new_df)
#Print the predictions
print(predictions2)
#install and import necessary libraries
library(readr) #For reading CSV files
#Import data
hersdata <- read.csv("C:\\Users\\acrot\\Downloads\\hersdata-1.csv")
#View structure of the data
str(hersdata)
head(hersdata)
#Remove rows with missing data
hersdata <- na.omit(hersdata)
#View structure of the data
str(hersdata)
head(hersdata)
#Fit the multiple linear regression model
model <- lm(LDL ~ BMI + AGE, data = hersdata)
#Perform an overall test
overall_test <- linearHypothesis(model, c("BMI = 0", "AGE = 0"))
#Display the results
summary(overall_test)
install.packages("dplyr")
install.packages("car")
library(readr) #For reading CSV files
library(dplyr)   #For data manipulation
library(car)    # For overall test
#Fit the multiple linear regression model
model <- lm(LDL ~ BMI + AGE, data = hersdata)
#Perform an overall test
overall_test <- linearHypothesis(model, c("BMI = 0", "AGE = 0"))
#Display the results
summary(overall_test)
install.packages("tidyr")
library(tidyr)  #For removal of missing data
#Remove rows with missing data
hersdata <- hersdata %>% drop_na(LDL, BMI, AGE)
#Import data
hersdata <- read.csv("C:\\Users\\acrot\\Downloads\\hersdata-1.csv")
#Remove rows with missing data
hersdata <- hersdata %>% drop_na(LDL, BMI, AGE)
install.packages("stats")
install.packages("stats")
library(stats)    # For overall test
#Fit the multiple linear regression model
model <- lm(LDL ~ BMI + AGE, data = hersdata)
#Perform an overall test using ANOVA
overall_test <- anova(model)
#Display the results
print(overall_test)
#Perform the t-test
t_test <- t.test(hersdata$BMI, hersdata$LDL, alternative = "two.sided", paired = FALSE)
#Print the result
print(t_test)
#Create a reduced model
reduced <- lm(LDL ~ Age, data = hersdata)
#Create a full model
full <- lm(LDL ~ BMI + age, data = hersdata)
#Perform a partial F-test using ANOVA
partial_F <- anova(reduced,full)
#Print the result
print(partial_F)
#Create a reduced model
reduced <- lm(LDL ~ AGE, data = hersdata)
#Create a full model
full <- lm(LDL ~ BMI + AGE, data = hersdata)
#Perform a partial F-test using ANOVA
partial_F <- anova(reduced,full)
#Print the result
print(partial_F)
#Create a reduced model for groups of predictors
reduced_groups <- lm(LDL ~ STATINS + AGE + SMOKING + DRINKANY + NONWHITE, data = hersdata)
#Create a full model for groups of predictors
full_groups <- lm(LDL ~ STATINS + BMI + STATINS:BMI + AGE + SMOKING + DRINKANY + NONWHITE, data = hersdata)
#Perform a partial F-test using ANOVA
partial_F_groups <- anova(reduced_groups,full_groups)
#Print the result
print(partial_F_groups)
library(readr) #For reading CSV files
library(dplyr)   #For data manipulation
library(stats)    # For overall test, t-test
pollution <- read.csv("C:\\Users\\acrot\\Downloads\\pollution.csv")
patsat <- read.csv("C:\\Users\\acrot\\Downloads\\patsat.csv")
#View structure of the data
str(pollution)
head(pollution)
#Create the Main Effect model
model_main <- lm(nn ~ caps + so2, data = pollution)
summary(model_main)
#Create the Interaction model
model_interaction <- lm(nn ~ caps + so2 + (caps*so2), data = pollution)
summary(model_interaction)
#Create the CAPs-only model
model_caps <- lm(nn ~ caps, data = pollution)
summary(model_caps)
# Test the correlation between Nn and CAPs
cor_nncaps <- cor.test(pollution$nn, pollution$caps)
# Test the correlation between Nn and SO2
cor_nnso2 <- cor.test(pollution$nn, pollution$so2)
# Test the correlation between CAPs and SO2
cor_capsso2 <- cor.test(pollution$caps, pollution$so2)
# Print the correlation test results
print(cor_nncaps)
print(cor_nnso2)
print(cor_capsso2)
#Fit the regression model
model_patsat <- lm(Y ~ X1 + X2 + X3, data = patsat)
summary(model_patsat)
#Get 95% CI estimates for each parameter
confint(model_patsat, level = 0.95)
#Get 95% CI estimates for specific paramaters
predict(model_patsat,data.frame(X1 = 35, X2 = 45, X3 = 2.2), interval = "confidence")
#install and import necessary libraries
#install.packages("readr")
#install.packages("dplyr")
#install.packages("tidyr")
#install.packages("stats")
library(readr) #For reading CSV files
library(dplyr)   #For data manipulation
library(tidyr)  #For removal of missing data
library(stats)    # For overall test, t-test
#Import data
patsat <- read.csv("C:\\Users\\acrot\\Downloads\\patsat.csv")
#View structure of the data
str(patsat)
head(patsat)
#Fit the regression model
model_patsat <- lm(Y ~ X1 + X2 + X3, data = patsat)
summary(model_patsat)
#Create the full model
model_full <- lm(Y ~ X1 + X2 + X3, data = patsat)
summary(model_full)
#Create the reduced model
model_reduced <- lm(Y ~ X1 + X2, data = patsat)
summary(model_reduced)
#Perform an ANOVA to compared full model and reduced model
anova_x3 <- anova(model_full, model_reduced)
print(anova_x3)
View(model_reduced)
#Create further reduced model
model_onlyx1 <- lm(Y ~ X1, data = patsat)
summary(model_onlyx1)
#Perform an ANOVA to compared full model and further reduced model
anova_further <- anova(model_full, model_onlyx1)
print(anova_further)
install.packages(c("insight", "knitr", "minqa", "openssl", "purrr", "quantreg", "rmarkdown", "rstudioapi", "sass", "tinytex", "xfun", "xml2"))
# 1a - What are the dimensions of this data?
dim(smarket)
# Initialize workspace
rm(list = ls())
# Import data
smarket <- read.csv("C:\\Users\\acrot\\OneDrive\\Documents\\ADS-635\\Smarket.csv")
# 1a - What are the dimensions of this data?
dim(smarket)
# 1b - What are the averages for: Lag1 – Lag5? Calculate this in two different ways.
# Method 1 - use the mean() function
mean(smarket$Lag1)
# 1b - What are the averages for: Lag1 – Lag5? Calculate this in two different ways.
# Method 1 - use the mean() function
mean(smarket$Lag1)
mean(smarket$Lag2)
mean(smarket$Lag3)
mean(smarket$Lag4)
mean(smarket$Lag5)
# Method 2 - Using the summary() function
summary(smarket$Lag1)
summary(smarket$Lag2)
summary(smarket$Lag3)
summary(smarket$Lag4)
summary(smarket$Lag5)
print(typeof(smarket$Direction))
table(smarket$Direction)
rm(list = ls())
setwd("C:\\Users\\acrot\\OneDrive\\Documents\\ADS-636")
data(state)
?state
summary(state)
library(glasso)
graphics.off()
library(gRbase)
library(gRim)
library(gRain)
library(graph)
library(corrplot)
library(igraph)
M <- cor(state.x77)
corrplot(M)
force(state.region)
force(state.name)
force(state.division)
force(state.center)
force(state.area)
force(state.abb)
dats <- state.x77[ , -1]
new_dats <- dats[-c(39,42), ]
S.body <- cov.wt(new_dats, method = "ML")
PC.body <- cov2pcor(S.body$cov)
diag(PC.body) <- 0
heatmap(PC.body)
S <- S.body$cov
my_rhos <- c(2,5,10,15,25,50)
graph_list <- list()
names_list <- list()
for (rho in my_rhos) {
fit <- glasso(S, rho = rho)
edges <- fit$wi != 0
diag(edges) <- 0
graph_list[[as.character(rho)]] <- graph_from_adjacency_matrix(edges, mode = "undirected")
names_list[[as.character(rho)]] <- names(new_dats)
}
pdf("GGM_graphs.pdf")
for (i in 1:length(graph_list)) {
V(graph_list[[i]])$name <- names_list[[i]]
plot(graph_list[[i]], main = paste("Graphical Lasso with rho =", names(graph_list)[i]))
}
View(new_dats)
View(new_dats)
View(new_dats)
View(new_dats)
new_dats$names
# Initialize workspace
rm(list = ls())
setwd("C:\\Users\\acrot\\OneDrive\\Documents\\ADS-636")
# Import and preview the data
data(state)
?state
graphics.off()
# View a correlation matrix
M <- cor(state.x77)
corrplot(M)
force(state.region)
force(state.name)
force(state.division)
force(state.center)
force(state.abb)
force(state.area)
# Prepare data
dats <- state.x77[ , -1]
new_dats <- dats[-c(39,42), ]
# Compute partial correlation
S.body <- cov.wt(new_dats, method = "ML")
S <- S.body$cov
# Fit the model using Graphical Lasso with multiple penalties
my_rhos <- c(2,5,10,15,25,50)
graph_list <- list()
for (rho in my_rhos) {
fit <- glasso(S, rho = rho)
edges <- fit$wi != 0
diag(edges) <- 0
g <- graph_from_adjacency_matrix(edges, mode = "undirected")
V(g)$name <- names(new_dats)
graph_list[[as.character(rho)]] <- g
}
# Fit the model using Graphical Lasso with multiple penalties
my_rhos <- c(2,5,10,15,25,50)
graph_list <- list()
for (rho in my_rhos) {
fit <- glasso(S, rho = rho)
edges <- fit$wi != 0
diag(edges) <- 0
g <- graph_from_adjacency_matrix(edges, mode = "undirected")
if (vcount(g) > 0) {  # Ensure the graph has vertices
V(g)$name <- names(new_dats)
}
graph_list[[as.character(rho)]] <- g
}
for (rho in my_rhos) {
fit <- glasso(S, rho = rho)
edges <- fit$wi != 0
diag(edges) <- 0
g <- graph_from_adjacency_matrix(edges, mode = "undirected", diag = FALSE)
V(g)$name <- colnames(S)  # Assign column names of S as vertex names
graph_list[[as.character(rho)]] <- g
}
# Plot and analyze the graphs
pdf("GGM_graphs.pdf")
for (i in 1:length(graph_list)) {
plot(graph_list[[i]], main = paste("Graphical Lasso with rho =", names(graph_list)[i]))
}
dev.off()
data <- scale(new_dats)
# Define SOM grid size
som_grid <- somgrid(xdim = 5, ydim = 5, topo = "rectangular")
library(kohonen)
# Define SOM grid size
som_grid <- somgrid(xdim = 5, ydim = 5, topo = "rectangular")
# Train SOM model
som_model <- som(data, grid = som_grid, rlen = 100, alpha = c(0.05, 0.01))
plot(som_model, type = "codes", main = "SOM Codes")
plot(som_model, type = "count", main = "SOM Count")
plot(som_model, type = "mapping", main = "SOM Mapping")
# Matrix of codebook vectors
som_model$codes
# Mapping of each observation to the units
som_model$unit.classif
rm(list = ls())
setwd("C:\\Users\\acrot\\OneDrive\\Documents\\ADS-636")
data(iris)
?iris
force(iris)
# Agglomerative clustering function
agglomerative_clustering <- function(dissimilarity,
method = c("single", "average",
"complete")) {
method <- match.arg(method)
n <- nrow(dissimilarity)
clusters <- list()
for (i in 1:n) {
clusters[[i]] <- i
}
while (length(clusters) > 1) {
# Find the closest pair of clusters
min_dist <- Inf
merge <- c(0, 0)
for (i in 1:(length(clusters) - 1)) {
for (j in (i + 1):length(clusters)) {
dist <- dissimilarity[clusters[[i]], clusters[[j]], drop = FALSE]
# Single Linkage
if (method == "single") {
d <- min(dist)
# Average Linkage
} else if (method == "average") {
d <- mean(dist)
# Complete Linkage
} else if (method == "complete") {
d <- max(dist)
}
if (d < min_dist) {
min_dist <- d
merge <- c(i, j)
}
}
}
# Merge the closest pair of clusters
clusters[[merge[1]]] <- c(clusters[[merge[1]]], clusters[[merge[2]]])
clusters <- clusters[-merge[2]]
}
return(clusters[[1]])
}
dissimilarity <- dist(iris[, 1:4])
clusters_single <- agglomerative_clustering(dissimilarity, "single")
clusters_average <- agglomerative_clustering(dissimilarity, "average")
clusters_complete <- agglomerative_clustering(dissimilarity, "complete")
# Initialize workspace
rm(list = ls())
setwd("C:\\Users\\acrot\\OneDrive\\Documents\\ADS-636")
# Import and preview the data
data(iris)
?iris
# Agglomerative clustering function
agglomerative_clustering <- function(dissimilarity, method = "single") {
n <- nrow(dissimilarity)
clusters <- rep(1:n, each = 1)
history <- matrix(NA, nrow = n-1, ncol = 3)
for (step in 1:(n - 1)) {
# Find the closest pair of clusters
min_dist <- Inf
for (i in 1:(n-1)) {
for (j in (i+1):n) {
if (clusters[i] != clusters[j]) {
dist <- dissimilarity[i, j]
if (dist < min_dist) {
min_dist <- dist
merge <- c(i, j)
}
}
}
}
# Merge the closest pair of clusters
clusters[clusters == clusters[merge[2]]] <- clusters[merge[1]]
history[step, ] <- c(clusters[merge[1]], clusters[merge[2]], min_dist)
# Update the dissimilarity matrix based on the linkage method
if (method == "single") {
dissimilarity[clusters == clusters[merge[1]], clusters == clusters[merge[1]]] <- Inf
for (k in which(clusters != clusters[merge[1]])) {
d <- min(dissimilarity[c(merge[1], merge[2]), k])
dissimilarity[clusters[merge[1]], k] <- d
dissimilarity[k, clusters[merge[1]]] <- d
}
}
# Add similar updates for average and complete linkage methods
}
return(list(clusters = clusters, history = history))
}
# Example usage with the iris dataset
data(iris)
dissimilarity <- as.matrix(dist(iris[, 1:4]))
clusters_single <- agglomerative_clustering(dissimilarity, "single")
clusters_average <- agglomerative_clustering(dissimilarity, "average")
clusters_complete <- agglomerative_clustering(dissimilarity, "complete")
plot(hclust(as.dist(dissimilarity), method = "single"), main = "Single Linkage")
plot(hclust(as.dist(dissimilarity), method = "average"), main = "Average Linkage")
plot(hclust(as.dist(dissimilarity), method = "complete"), main = "Complete Linkage")
# Agglomerative clustering function
agglomerative_clustering <- function(dissimilarity, method = "single") {
# Get num of observations, initialize clusters, and matrix to record merge history
n <- nrow(dissimilarity)
clusters <- rep(1:n, each = 1)
history <- matrix(NA, nrow = n-1, ncol = 3)
# Iteratively merge clusters
for (step in 1:(n - 1)) {
# Find the closest pair of clusters
min_dist <- Inf
# Loop over all clusters
for (i in 1:(n-1)) {
for (j in (i+1):n) {
# Ensure comparisons are between different clusters
if (clusters[i] != clusters[j]) {
dist <- dissimilarity[i, j]
# Update minimum distance and remaining clusters
if (dist < min_dist) {
min_dist <- dist
merge <- c(i, j)
}
}
}
}
# Merge the closest pair of clusters and record in history matrix
clusters[clusters == clusters[merge[2]]] <- clusters[merge[1]]
history[step, ] <- c(clusters[merge[1]], clusters[merge[2]], min_dist)
# Update the dissimilarity matrix based on the method
if (method == "single") {
dissimilarity[clusters == clusters[merge[1]], clusters == clusters[merge[1]]] <- Inf
for (k in which(clusters != clusters[merge[1]])) {
d <- min(dissimilarity[c(merge[1], merge[2]), k])
dissimilarity[clusters[merge[1]], k] <- d
dissimilarity[k, clusters[merge[1]]] <- d
}
}
if (method == "average") {
dissimilarity[clusters == clusters[merge[1]], clusters == clusters[merge[1]]] <- Inf
for (k in which(clusters != clusters[merge[1]])) {
d <- mean(dissimilarity[c(merge[1], merge[2]), k])
dissimilarity[clusters[merge[1]], k] <- d
dissimilarity[k, clusters[merge[1]]] <- d
}
}
if (method == "complete") {
dissimilarity[clusters == clusters[merge[1]], clusters == clusters[merge[1]]] <- Inf
for (k in which(clusters != clusters[merge[1]])) {
d <- max(dissimilarity[c(merge[1], merge[2]), k])
dissimilarity[clusters[merge[1]], k] <- d
dissimilarity[k, clusters[merge[1]]] <- d
}
}
}
return(list(clusters = clusters, history = history))
}
dissimilarity <- as.matrix(dist(iris[, 1:4]))
clusters_single <- agglomerative_clustering(dissimilarity, "single")
clusters_average <- agglomerative_clustering(dissimilarity, "average")
clusters_complete <- agglomerative_clustering(dissimilarity, "complete")
# Plotting the results
plot(hclust(as.dist(dissimilarity), method = "single"), main = "Single Linkage")
plot(hclust(as.dist(dissimilarity), method = "average"), main = "Average Linkage")
plot(hclust(as.dist(dissimilarity), method = "complete"), main = "Complete Linkage")
plot(hclust(as.dist(dissimilarity), method = "single"), main = "Single Linkage")
rect.hclust(hc_som, k = 3)  # Drawing rectangles for k=3 clusters
library(ISLR)
rect.hclust(hc_som, k = 3)  # Drawing rectangles for k=3 clusters
rect.hclust(hc_som, k = 3)  # Drawing rectangles for k=3 clusters
library(cluster)
rect.hclust(hc_som, k = 3)  # Drawing rectangles for k=3 clusters
rect.hclust(as.dist(dissimilarity), method = "single", main = "Single Linkage", k = 3)  # Drawing rectangles for k=3 clusters
plot(rect.hclust(as.dist(dissimilarity), method = "single"), main = "Single Linkage", k = 3)
single <- hclust(as.dist(dissimilarity), method = "single")
plot(single, main = "Single Linkage")
rect.hclust(single, k = 3)  # Drawing rectangles for k=3 clusters
rect.hclust(single, k = 4)
plot(single, main = "Single Linkage")
rect.hclust(single, k = 3)
average <- hclust(as.dist(dissimilarity), method = "average")
plot(average, main = "Average Linkage")
rect.hclust(average, k = 3)
complete <- hclust(as.dist(dissimilarity), method = "complete")
plot(complete, main = "Complete Linkage")
rect.hclust(complete, k = 3)
